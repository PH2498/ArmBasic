"""
è¯­éŸ³ AI äº¤äº’ï¼šè¯­éŸ³è¾“å…¥ -> å¤§æ¨¡å‹ -> è¯­éŸ³è¾“å‡ºï¼Œå¹¶æ”¯æŒè°ƒç”¨äººè„¸è¯†åˆ«ï¼ˆå¦‚é—®ã€Œæˆ‘æ˜¯è°ã€æ—¶ç”¨æ‘„åƒå¤´è¯†åˆ«ï¼‰ã€‚
æ¨èä½¿ç”¨å›½å†…å…è´¹/ä½æˆæœ¬å¤§æ¨¡å‹ï¼šé€šä¹‰åƒé—®ï¼ˆé˜¿é‡Œï¼‰ï¼Œæ–°ç”¨æˆ·æœ‰å…è´¹é¢åº¦ã€‚
"""
import os
import sys
import re
import time
import cv2
import random
import subprocess
import signal
import threading
import queue
import shutil

# å°†é¡¹ç›®æ ¹ç›®å½•åŠ å…¥è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ FaceRecognitionModule
_PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if _PROJECT_ROOT not in sys.path:
    sys.path.insert(0, _PROJECT_ROOT)

# è¯­éŸ³è¯†åˆ«
try:
    import speech_recognition as sr
    import pyaudio  # noqa: F401
    HAS_PYAUDIO = True
except ImportError:
    sr = None
    HAS_PYAUDIO = False

# Whisper
try:
    import whisper
    HAS_WHISPER = True
except ImportError:
    HAS_WHISPER = False

# é€šä¹‰åƒé—®
try:
    import dashscope
    from dashscope import Generation, MultiModalConversation
    HAS_DASHSCOPE = True
except ImportError:
    HAS_DASHSCOPE = False

# è¯­éŸ³åˆæˆ
try:
    import edge_tts
    import asyncio
    HAS_EDGE_TTS = True
except ImportError:
    HAS_EDGE_TTS = False


# --- è¾…åŠ©å‡½æ•° ---

def _get_face_names():
    """è°ƒç”¨äººè„¸è¯†åˆ«æ¨¡å—ï¼Œè¿”å›å½“å‰æ‘„åƒå¤´ç”»é¢ä¸­è¯†åˆ«åˆ°çš„å§“ååˆ—è¡¨ã€‚"""
    try:
        from FaceRecognitionModule.run_face_recognition import recognize_faces_from_camera
        names = recognize_faces_from_camera()
        return names if names else []
    except Exception as e:
        print(f"âš ï¸ äººè„¸è¯†åˆ«è°ƒç”¨å¤±è´¥: {e}")
        return []


def _need_face_context(text):
    if not text or not text.strip():
        return False
    t = text.strip()
    patterns = [
        r"æˆ‘æ˜¯è°", r"æˆ‘æ˜¯è°\s*[ï¼Ÿ?]?", r"çœ‹çœ‹æˆ‘æ˜¯è°", r"è¯†åˆ«.*æˆ‘",
        r"è°åœ¨(é•œå¤´|æ‘„åƒå¤´|ç”»é¢)", r"ä½ .*(è®¤|è¯†).*æˆ‘", r"æˆ‘å«ä»€ä¹ˆ",
        r"çŸ¥é“æˆ‘æ˜¯è°", r"è®¤å‡ºæˆ‘", r"é•œå¤´.*è°",
        r"(èƒ½|å¯ä»¥)?çœ‹(åˆ°|è§)æˆ‘å—", r"(èƒ½|å¯ä»¥)?çœ‹å¾—åˆ°æˆ‘å—", r"(èƒ½|å¯ä»¥)?çœ‹è§æˆ‘å—",
        r"çœ‹ä¸€?ä¸‹?æˆ‘", r"æ‰“å¼€æ‘„åƒå¤´", r"è¯†åˆ«ä¸€ä¸‹æˆ‘", r"çœ‹çœ‹æœ‰æ²¡æœ‰äºº",
    ]
    for p in patterns:
        if re.search(p, t):
            return True
    return False


def _need_vision_context(text):
    if not text:
        return False
    t = text.strip()
    if _need_face_context(t):
        return False
    patterns = [
        r"çœ‹.*(æ‰‹é‡Œ|æ‹¿|ä»€ä¹ˆ)", r"è¿™æ˜¯ä»€ä¹ˆ", r"æè¿°.*(ç”»é¢|åœºæ™¯|å›¾ç‰‡)",
        r"ä½ çœ‹", r"å¸®æˆ‘çœ‹", r"è¯†åˆ«.*(ç‰©ä½“|ä¸œè¥¿)", r"ç¯å¢ƒ.*(æ€ä¹ˆæ ·|ä»€ä¹ˆæ ·)",
        r"è¯».*(æ–‡å­—|å­—)", r"æ‘„åƒå¤´.*(æ‹|çœ‹)",
        r"æ‰‹é‡Œ.*(æ‹¿|æ˜¯).*", r".*æ‹¿çš„.*ä»€ä¹ˆ.*",
    ]
    for p in patterns:
        if re.search(p, t):
            return True
    return False


def _capture_image_file():
    """æ‰“å¼€æ‘„åƒå¤´æ‹ä¸€å¼ ç…§ç‰‡ï¼Œä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶å¹¶è¿”å›è·¯å¾„ã€‚"""
    idx = -1
    for i in [1, 2, 0, 3]:
        cap = cv2.VideoCapture(i)
        if cap.isOpened():
            idx = i
            break
        cap.release()
    
    if idx < 0:
        print("âš ï¸ æœªæ‰¾åˆ°å¯ç”¨æ‘„åƒå¤´")
        return None

    print(f"ğŸ“· æ­£åœ¨æ‹ç…§ (Camera {idx})...")
    cap = cv2.VideoCapture(idx)
    # é¢„çƒ­
    for _ in range(15):
        cap.read()
        time.sleep(0.05)
    
    ret, frame = cap.read()
    cap.release()
    
    if not ret or frame is None:
        print("âš ï¸ æ‹ç…§å¤±è´¥")
        return None
    
    import tempfile
    f = tempfile.NamedTemporaryFile(suffix=".jpg", delete=False)
    f.close()
    cv2.imwrite(f.name, frame)
    return f.name


def _get_env_path():
    return os.path.join(os.path.dirname(os.path.abspath(__file__)), ".env")


def _load_api_key():
    key = os.environ.get("DASHSCOPE_API_KEY", "").strip()
    env_path = _get_env_path()
    if not key and os.path.isfile(env_path):
        with open(env_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line.startswith("DASHSCOPE_API_KEY="):
                    key = line.split("=", 1)[1].strip().strip('"\'')
                    if key and "xxxxxxxx" not in key:
                        break
                    key = ""
    return key


def chat_with_llm(user_text, face_context=None, image_path=None, stream=False):
    """è°ƒç”¨å¤§æ¨¡å‹å¾—åˆ°å›å¤ã€‚æ”¯æŒæµå¼è¿”å›ã€‚"""
    api_key = _load_api_key()
    if not HAS_DASHSCOPE:
        return (None, "æœªå®‰è£… dashscope") if not stream else iter([])
    if not api_key:
        return (None, "æœªé…ç½® API Key") if not stream else iter([])

    dashscope.api_key = api_key

    # è§†è§‰å¤šæ¨¡æ€ (æš‚ä¸æ”¯æŒæµå¼ï¼Œå› ä¸º MultiModalConversation æµå¼æ¥å£è¾ƒå¤æ‚ä¸” VL ç”Ÿæˆè¾ƒå¿«)
    if image_path:
        if stream: print("âš ï¸ è§†è§‰æ¨¡å¼æš‚ä¸æ”¯æŒæµå¼ï¼Œå°†è½¬ä¸ºä¸€æ¬¡æ€§è¿”å›")
        print(f"ğŸ–¼ï¸ æ­£åœ¨è°ƒç”¨è§†è§‰æ¨¡å‹ (qwen-vl-max)...")
        messages = [
            {
                "role": "user",
                "content": [
                    {"image": f"file://{image_path}"},
                    {"text": user_text if user_text else "è¿™å¼ å›¾é‡Œæœ‰ä»€ä¹ˆï¼Ÿ"}
                ]
            }
        ]
        try:
            resp = MultiModalConversation.call(model='qwen-vl-max', messages=messages)
            if resp.status_code == 200:
                content_list = resp.output.choices[0].message.content
                text_reply = ""
                for item in content_list:
                    if "text" in item:
                        text_reply += item["text"]
                return (text_reply.strip(), None) if not stream else iter([text_reply.strip()])
            else:
                return (None, resp.message or "è§†è§‰æ¨¡å‹å¼‚å¸¸") if not stream else iter([])
        except Exception as e:
            return (None, f"è§†è§‰æ¨¡å‹å¤±è´¥: {e}") if not stream else iter([])

    # æ–‡æœ¬/äººè„¸æ¨¡å¼
    system = "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„è¯­éŸ³åŠ©æ‰‹ï¼Œç”¨ç®€çŸ­å£è¯­åŒ–ä¸­æ–‡å›ç­”ã€‚"
    if face_context:
        system += f"\nã€æ‘„åƒå¤´äººè„¸ã€‘{face_context}ã€‚"
    messages = [{"role": "system", "content": system}, {"role": "user", "content": user_text}]
    
    try:
        if stream:
            # æµå¼è°ƒç”¨
            # Qwen çš„ stream=True è¿”å›çš„æ˜¯ iteratorï¼Œæ¯æ¬¡ output.text æ˜¯å…¨é‡æ–‡æœ¬(appendæ¨¡å¼)è¿˜æ˜¯å¢é‡ï¼Ÿ
            # ç»ç¡®è®¤ï¼Œqwen-turbo stream=True æ—¶ï¼Œoutput.text æ˜¯ *å…¨é‡* æ–‡æœ¬ã€‚éœ€è¦è‡ªå·± diffã€‚
            # ä½† generation ä¹Ÿæœ‰ incremental_output=True é€‰é¡¹ (éƒ¨åˆ†æ¨¡å‹æ”¯æŒ)
            # ä¸ºå…¼å®¹æ€§ï¼Œè¿™é‡Œæ‰‹åŠ¨ diff
            def _generator():
                responses = Generation.call(model="qwen-turbo", messages=messages, result_format='message', stream=True, incremental_output=True)
                for resp in responses:
                    if resp.status_code == 200:
                        # incremental_output=True: output.choices[0].message.content æ˜¯å¢é‡
                        # å¦‚æœä¸æ”¯æŒ incrementalï¼Œåˆ™ output.text æ˜¯å…¨é‡ã€‚
                        # qwen-turbo æ”¯æŒ incremental_output=True
                        content = resp.output.choices[0].message.content
                        if content:
                            yield content
                    else:
                        print(f"Model Error: {resp.message}")
            return _generator()
        else:
            resp = Generation.call(model="qwen-turbo", messages=messages)
            if resp.status_code == 200 and resp.output and resp.output.get("text"):
                return resp.output["text"].strip(), None
            return None, resp.message or "æ¨¡å‹å¼‚å¸¸"
    except Exception as e:
        return (None, str(e)) if not stream else iter([])


# --- æ ¸å¿ƒç±» ---

class AudioPlayer:
    """å¼‚æ­¥éŸ³é¢‘æ’­æ”¾å™¨ï¼Œæ”¯æŒæµå¼æ’­æ”¾ä¸æ‰“æ–­ã€‚"""
    def __init__(self):
        self._play_thread = None
        self._stop_event = threading.Event()
        self._current_process = None
        self._temp_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "tts_cache")
        # æ¸…ç†æ—§ç¼“å­˜
        if os.path.exists(self._temp_dir):
            try: shutil.rmtree(self._temp_dir)
            except: pass
        os.makedirs(self._temp_dir, exist_ok=True)

    def play_stream(self, text_generator, voice="zh-CN-YunyangNeural", rate="+10%", pitch="-5Hz", blocking=True):
        """
        æ¥æ”¶æ–‡æœ¬æµï¼ˆç”Ÿæˆå™¨ï¼‰ï¼Œå®æ—¶åˆæˆå¹¶æ’­æ”¾ã€‚
        :param text_generator: äº§å‡ºæ–‡æœ¬ç‰‡æ®µçš„ç”Ÿæˆå™¨ (iterator)
        """
        if not HAS_EDGE_TTS: return

        # åœæ­¢ä¸Šä¸€æ¬¡
        self.stop()
        self._stop_event.clear()

        # é˜Ÿåˆ—
        q = queue.Queue()

        # ç”Ÿäº§è€…ï¼šä» generator è¯»å–æ–‡æœ¬ -> ç¼“å†² -> æŒ‰å¥åˆ‡åˆ† -> TTS -> é˜Ÿåˆ—
        def producer():
            buffer = ""
            idx = 0
            
            # æ­£åˆ™ï¼šåŒ¹é…æ ‡ç‚¹ç¬¦å·
            split_pattern = r'([ã€‚ï¼ï¼Ÿï¼›!?;]+)'
            
            try:
                for chunk in text_generator:
                    if self._stop_event.is_set(): break
                    if not chunk: continue
                    
                    buffer += chunk
                    
                    # å°è¯•åˆ‡åˆ†
                    while True:
                        # æ‰¾ç¬¬ä¸€ä¸ªæ ‡ç‚¹
                        match = re.search(split_pattern, buffer)
                        if not match:
                            break
                        
                        end_pos = match.end()
                        sentence = buffer[:end_pos]
                        buffer = buffer[end_pos:]
                        
                        clean = self._clean_text(sentence)
                        if clean:
                            _gen_audio(clean, idx)
                            idx += 1
                
                # å¤„ç†å‰©ä½™æ–‡æœ¬
                if buffer and not self._stop_event.is_set():
                    clean = self._clean_text(buffer)
                    if clean:
                        _gen_audio(clean, idx)
            
            except Exception as e:
                print(f"âš ï¸ æµå¼å¤„ç†å¼‚å¸¸: {e}")
            finally:
                q.put(None)

        def _gen_audio(text, i):
            filename = f"tts_stream_{int(time.time())}_{i}.mp3"
            filepath = os.path.join(self._temp_dir, filename)
            
            async def _run_tts():
                # å¢åŠ è¯­é€Ÿå’ŒéŸ³è°ƒå‚æ•°
                com = edge_tts.Communicate(text=text, voice=voice, rate=rate, pitch=pitch)
                await com.save(filepath)
            
            try:
                asyncio.run(_run_tts())
                q.put(filepath)
            except Exception as e:
                print(f"âš ï¸ TTSç”Ÿæˆå¤±è´¥: {e}")

        # æ¶ˆè´¹è€…ï¼ˆåŒ playï¼‰
        def consumer():
            while not self._stop_event.is_set():
                try:
                    filepath = q.get(timeout=0.1)
                    if filepath is None: break
                except queue.Empty:
                    # å¦‚æœç”Ÿäº§è€…æ´»ç€ï¼Œç»§ç»­ç­‰ï¼›æ­»äº†ä¸”ç©ºäº†ï¼Œé€€å‡º
                    if t_prod.is_alive(): continue
                    else: break
                
                if self._stop_event.is_set(): break

                # æ’­æ”¾
                if sys.platform == "darwin":
                    self._current_process = subprocess.Popen(
                        ['afplay', filepath],
                        stdout=subprocess.DEVNULL,
                        stderr=subprocess.DEVNULL
                    )
                    self._current_process.wait()
                    self._current_process = None
                else:
                    print(f"ğŸ”Š {filepath}...")

                try: os.remove(filepath)
                except: pass

        t_prod = threading.Thread(target=producer)
        t_cons = threading.Thread(target=consumer)
        
        t_prod.start()
        t_cons.start()

        if blocking:
            t_cons.join()
        else:
            self._play_thread = t_cons

    def play(self, text, voice="zh-CN-YunxiNeural", blocking=True):
        """æµå¼æ’­æ”¾è¯­éŸ³ã€‚"""
        if not text or not HAS_EDGE_TTS:
            return
        self.stop()
        self._stop_event.clear()
        
        # ç®€å•æ¸…æ´—
        clean_text = self._clean_text(text)
        if not clean_text:
            return

        # é¢„åˆ†æ®µï¼ˆæŒ‰æ ‡ç‚¹ï¼‰
        parts = re.split(r'([ã€‚ï¼ï¼Ÿï¼›!?;]+)', clean_text)
        chunks = []
        current_chunk = ""
        for p in parts:
            current_chunk += p
            if re.search(r'[ã€‚ï¼ï¼Ÿï¼›!?;]', p):
                chunks.append(current_chunk)
                current_chunk = ""
        if current_chunk:
            chunks.append(current_chunk)
            
        # ç”Ÿäº§è€…ï¼šç”ŸæˆéŸ³é¢‘æ–‡ä»¶
        q = queue.Queue()
        
        def producer():
            for i, chunk in enumerate(chunks):
                if self._stop_event.is_set(): break
                if not chunk.strip(): continue
                
                filename = f"tts_{int(time.time())}_{i}.mp3"
                filepath = os.path.join(self._temp_dir, filename)
                
                async def _gen():
                    # ç»Ÿä¸€ä½¿ç”¨é…ç½®å¥½çš„è¯­é€Ÿå’ŒéŸ³è°ƒ
                    com = edge_tts.Communicate(text=chunk, voice=voice, rate="+10%", pitch="-5Hz")
                    await com.save(filepath)
                
                try:
                    asyncio.run(_gen())
                    q.put(filepath)
                except Exception as e:
                    print(f"âš ï¸ TTS ç”Ÿæˆå¤±è´¥: {e}")
            q.put(None) # ç»“æŸæ ‡å¿—

        # æ¶ˆè´¹è€…ï¼šæ’­æ”¾
        def consumer():
            while not self._stop_event.is_set():
                try:
                    filepath = q.get(timeout=0.5)
                    if filepath is None: break
                except queue.Empty:
                    if not t_prod.is_alive() and q.empty():
                        break
                    continue
                
                if self._stop_event.is_set(): break
                
                if sys.platform == "darwin":
                    # Mac ä½¿ç”¨ afplay
                    self._current_process = subprocess.Popen(
                        ['afplay', filepath],
                        stdout=subprocess.DEVNULL,
                        stderr=subprocess.DEVNULL
                    )
                    self._current_process.wait()
                    self._current_process = None
                else:
                    # Linux/Windows æš‚ç•¥ï¼Œå‡è®¾ Mac
                    print(f"ğŸ”Š {text[:10]}...")
                
                # æ’­æ”¾å®Œåˆ é™¤
                try:
                    os.remove(filepath)
                except:
                    pass

        t_prod = threading.Thread(target=producer)
        t_cons = threading.Thread(target=consumer)
        
        t_prod.start()
        t_cons.start()
        
        if blocking:
            t_cons.join()
        else:
            self._play_thread = t_cons

    def play_file(self, filepath, blocking=True):
        """æ’­æ”¾æœ¬åœ°æ–‡ä»¶ï¼ˆæ— å»¶è¿Ÿï¼‰ã€‚"""
        if not os.path.exists(filepath):
            return
        self.stop()
        self._stop_event.clear()
        
        if sys.platform == "darwin":
            self._current_process = subprocess.Popen(
                ['afplay', filepath],
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL
            )
            if blocking:
                self._current_process.wait()
                self._current_process = None
            else:
                # å¦‚æœéé˜»å¡ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªçº¿ç¨‹æ¥ç­‰å¾…å®ƒç»“æŸï¼ˆæˆ–è€…å°±ä¸ç®¡äº†ï¼Œä½†ä¸ºäº†èƒ½stopï¼Œæœ€å¥½è®°å½•ï¼‰
                # ç®€å•èµ·è§ï¼Œéé˜»å¡æ¨¡å¼ä¸‹æˆ‘ä»¬åªè®°å½• processï¼Œä¸join
                pass

    def stop(self):
        """åœæ­¢æ’­æ”¾ã€‚"""
        self._stop_event.set()
        
        # ç»ˆæ­¢å½“å‰æ’­æ”¾è¿›ç¨‹
        if self._current_process:
            if self._current_process.poll() is None:
                self._current_process.terminate()
                try:
                    self._current_process.wait(timeout=0.1)
                except:
                    self._current_process.kill()
            self._current_process = None
        
        # ç­‰å¾…æ’­æ”¾çº¿ç¨‹ç»“æŸ
        if self._play_thread and self._play_thread.is_alive():
            self._play_thread.join(timeout=0.2)

        try:
            for name in os.listdir(self._temp_dir):
                if name.endswith(".mp3"):
                    try:
                        os.remove(os.path.join(self._temp_dir, name))
                    except:
                        pass
        except:
            pass

    def is_playing(self):
        return self._play_thread and self._play_thread.is_alive()

    def _clean_text(self, text):
        if not text:
            return ""
        t = re.sub(r'[\U00010000-\U0010ffff]', '', text)
        t = t.replace("*", "").replace("#", "").replace("`", "")
        return t.strip()


class SpeechAssistant:
    def __init__(self):
        self.r = sr.Recognizer()
        self.r.dynamic_energy_threshold = True
        self.r.pause_threshold = 0.8
        self.r.phrase_threshold = 0.4
        self.r.non_speaking_duration = 0.4
        
        self.mic = None
        self.whisper_model = None
        self.player = AudioPlayer()
        self._llm_stop_event = threading.Event()
        
        # å”¤é†’è¯
        self.WAKE_WORD = "å°ç¬¨"
        
        # çŠ¶æ€
        self.is_active = False
        self.last_active_time = 0
        self.IDLE_TIMEOUT = 30
        
        self._init_mic()

    def _init_mic(self):
        """åˆå§‹åŒ–éº¦å…‹é£ä¸ç¯å¢ƒå™ªéŸ³ã€‚"""
        if self.mic: return
        print("ğŸ¤ åˆå§‹åŒ–éº¦å…‹é£...")
        self.mic = sr.Microphone(sample_rate=16000, chunk_size=1024)
        with self.mic as source:
            # ä»…æ ¡å‡†ä¸€æ¬¡
            print("ğŸ”‡ æ­£åœ¨æ ¡å‡†ç¯å¢ƒå™ªéŸ³ (è¯·ä¿æŒå®‰é™ 1ç§’)...")
            self.r.adjust_for_ambient_noise(source, duration=1)
            # æ ¡å‡†åå…³é—­åŠ¨æ€è°ƒæ•´ï¼Œé˜²æ­¢ AI è¯´è¯æ—¶é˜ˆå€¼æ¼‚ç§»
            self.r.dynamic_energy_threshold = False
            # å›ºå®šé˜ˆå€¼ï¼Œé¿å…å› ä¸ºæ‰¬å£°å™¨å¯¼è‡´é˜ˆå€¼æ¼‚ç§»
            self.r.energy_threshold = max(60, self.r.energy_threshold)
            print(f"âœ… æ ¡å‡†å®Œæˆ (é˜ˆå€¼: {self.r.energy_threshold:.0f})")

    def _init_static_audio(self):
        """é¢„ç”Ÿæˆé™æ€æç¤ºéŸ³ï¼Œæ¶ˆé™¤ç½‘ç»œå»¶è¿Ÿã€‚"""
        cache_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "static_audio")
        os.makedirs(cache_dir, exist_ok=True)
        
        # å®šä¹‰éœ€è¦ç¼“å­˜çš„é™æ€æ–‡æœ¬
        static_texts = {
            "wake": "æˆ‘åœ¨ã€‚",
            "listen": "è¯·å©å’ã€‚",
            "bye": "å¥½çš„ï¼Œæ‹œæ‹œã€‚",
            "rest": "é‚£æˆ‘å…ˆä¼‘æ¯äº†ï¼Œæœ‰äº‹å«æˆ‘ã€‚",
            "error": "æŠ±æ­‰ï¼Œæˆ‘æ²¡å¬æ¸…ã€‚"
        }
        
        self.static_audio_files = {}
        
        # æ£€æŸ¥å¹¶ç”Ÿæˆ
        for key, text in static_texts.items():
            filepath = os.path.join(cache_dir, f"{key}.mp3")
            self.static_audio_files[key] = filepath
            if not os.path.exists(filepath):
                print(f"ğŸ› ï¸ ç”Ÿæˆé™æ€éŸ³é¢‘: {text}")
                try:
                    # ä½¿ç”¨ä¸ä¸»è¯­éŸ³ä¸€è‡´çš„éŸ³è‰²
                    async def _gen():
                        com = edge_tts.Communicate(text=text, voice="zh-CN-YunyangNeural", rate="+10%", pitch="-5Hz")
                        await com.save(filepath)
                    asyncio.run(_gen())
                except Exception as e:
                    print(f"âš ï¸ ç”Ÿæˆé™æ€éŸ³é¢‘å¤±è´¥: {e}")

    def _get_whisper(self):
        if not HAS_WHISPER: return None
        if not self.whisper_model:
            print("â³ åŠ è½½ Whisper æ¨¡å‹ (tiny)...")
            try:
                import torch
                # å°è¯•ä½¿ç”¨ MPS (Metal Performance Shaders) åŠ é€Ÿ
                device = "cpu"
                if torch.backends.mps.is_available():
                    device = "mps"
                    print("ğŸš€ ä½¿ç”¨ MPS åŠ é€Ÿæ¨ç†")
                self.whisper_model = whisper.load_model("tiny", device=device)
            except Exception as e:
                print(f"âš ï¸ MPS åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ° CPU: {e}")
                self.whisper_model = whisper.load_model("tiny", device="cpu")
        return self.whisper_model

    def listen(self, is_speaking=False):
        """ç›‘å¬å¹¶è¿”å›æ–‡æœ¬ã€‚æ”¯æŒæ‰“æ–­æ£€æµ‹ã€‚"""
        if not self.mic: return None
        
        # ä¼˜åŒ–æ‰“æ–­ï¼šAI è¯´è¯æ—¶ï¼Œä½¿ç”¨æçŸ­çš„çª—å£(1s)è¿›è¡Œåˆ‡ç‰‡ç›‘å¬
        phrase_limit = 0.8 if is_speaking else 8
        timeout = 0.6 if is_speaking else 6
        
        with self.mic as source:
            try:
                # pause_threshold: è¯´è¯ååœé¡¿å¤šä¹…ç®—ç»“æŸã€‚
                # æ­£å¸¸å¯¹è¯ 0.35s (æ›´å¿«)ï¼Œæ‰“æ–­æ—¶ 0.18s (æé€Ÿ)
                self.r.pause_threshold = 0.18 if is_speaking else 0.35
                
                # non_speaking_duration: å¤šå°‘ç§’é™éŸ³ç®—æ²¡äººè¯´è¯
                self.r.non_speaking_duration = 0.18
                
                audio = self.r.listen(source, timeout=timeout, phrase_time_limit=phrase_limit)
            except sr.WaitTimeoutError:
                return None
            except Exception as e:
                return None

        text = ""
        # 1. Whisper
        w_model = self._get_whisper()
        if w_model:
            try:
                import tempfile
                with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
                    f.write(audio.get_wav_data())
                    tmp = f.name
                res = w_model.transcribe(
                    tmp,
                    language="zh",
                    fp16=False,
                    beam_size=1,
                    best_of=1,
                    temperature=0.0,
                    condition_on_previous_text=False,
                    no_speech_threshold=0.6
                )
                text = res.get("text", "").strip()
                os.remove(tmp)
            except:
                pass
        
        # 2. Google fallback
        if not text:
            try:
                text = self.r.recognize_google(audio, language="zh-CN")
            except:
                pass
        
        return text.strip() if text else None

    def run(self):
        """ä¸»å¾ªç¯ã€‚"""
        self._init_mic()
        self._init_static_audio()
        
        print(f"\nâœ¨ {self.WAKE_WORD} è¯­éŸ³åŠ©æ‰‹å·²å°±ç»ª (Ctrl+C é€€å‡º)")
        print(f"ğŸ‘‰ è¯´ â€œ{self.WAKE_WORD}â€ å”¤é†’æˆ‘...")

        while True:
            try:
                # æ£€æŸ¥æ˜¯å¦è¶…æ—¶ä¼‘çœ 
                if self.is_active:
                    if time.time() - self.last_active_time > self.IDLE_TIMEOUT:
                        print("ğŸ’¤ è¶…è¿‡30ç§’æœªäº¤äº’ï¼Œè¿›å…¥å¾…æœºæ¨¡å¼...")
                        self.player.play_file(self.static_audio_files.get("rest"), blocking=True)
                        self.is_active = False
                
                # ç›‘å¬çŠ¶æ€
                is_playing = self.player.is_playing()
                text = self.listen(is_speaking=is_playing)
                
                if not text:
                    continue
                
                # æ‰“å°å¬åˆ°çš„å†…å®¹ (å¦‚æœæ˜¯æ’­æ”¾ä¸­ï¼Œå¯èƒ½å¬åˆ°è‡ªå·±ï¼Œä½œä¸ºè°ƒè¯•ä¿¡æ¯)
                if is_playing:
                    print(f"ğŸ‘‚ [æ’­æ”¾ä¸­ç›‘å¬] {text}")
                else:
                    print(f"ğŸ‘‚ {text}")

                # æ‰“æ–­æ£€æµ‹ä¸è‡ªå¬è¿‡æ»¤
                if is_playing:
                    # æ¨¡ç³ŠåŒ¹é…å”¤é†’è¯
                    is_wake = False
                    if self.WAKE_WORD in text:
                        is_wake = True
                    else:
                        # åŒéŸ³è¯æ¨¡ç³ŠåŒ¹é…
                        fuzzy_words = ["å°æœ¬", "æ ¡æœ¬", "æ™“ç¬¨", "å°å¥”", "ç¬¨ç¬¨", "å°è¹¦"]
                        for w in fuzzy_words:
                            if w in text:
                                is_wake = True
                                break
                    
                    if is_wake:
                        print(f"âš¡ï¸ è§¦å‘æ‰“æ–­ï¼")
                        self._llm_stop_event.set()
                        self.player.stop()
                        self.is_active = True
                        self.last_active_time = time.time()
                        # ç«‹å³æ’­æ”¾æœ¬åœ°ç¼“å­˜çš„â€œæˆ‘åœ¨â€ï¼Œæ— å»¶è¿Ÿ
                        self.player.play_file(self.static_audio_files.get("wake"), blocking=True)
                        continue
                    else:
                        # åªæœ‰å¬åˆ°å”¤é†’è¯æ‰ç®—æ‰“æ–­ï¼Œå¦åˆ™è§†ä¸ºè‡ªå¬ï¼ˆå¬åˆ°è‡ªå·±è¯´è¯ï¼‰
                        # print(f"ğŸ”‡ å¿½ç•¥è‡ªå¬/èƒŒæ™¯éŸ³: {text}")
                        continue
                
                # éæ’­æ”¾çŠ¶æ€çš„å¤„ç†
                if not self.is_active:
                    # å¾…æœºæ¨¡å¼ï¼šåªå“åº”å”¤é†’è¯
                    if self.WAKE_WORD in text or "å°æœ¬" in text or "ç¬¨ç¬¨" in text:
                        print("ğŸš€ è¢«å”¤é†’ï¼")
                        self.is_active = True
                        self.last_active_time = time.time()
                        # ç«‹å³æ’­æ”¾æœ¬åœ°ç¼“å­˜çš„â€œæˆ‘åœ¨â€
                        self.player.play_file(self.static_audio_files.get("wake"), blocking=True)
                else:
                    # æ´»è·ƒæ¨¡å¼
                    self.last_active_time = time.time()
                    
                    # é€€å‡ºæŒ‡ä»¤
                    if "å†è§" in text or "é€€ä¸‹" in text or "ä¼‘æ¯" in text:
                        self.player.play_file(self.static_audio_files.get("bye"), blocking=True)
                        self.is_active = False
                        continue
                    
                    self._handle_command(text)
                    # å…³é”®ä¿®æ”¹ï¼šAI å›å¤ç»“æŸåï¼Œå†æ¬¡æ›´æ–° last_active_time
                    # è¿™æ ·ä¼‘çœ å€’è®¡æ—¶æ‰ä¼šåœ¨ AI è¯´å®Œåå¼€å§‹ç®—
                    self.last_active_time = time.time()

            except KeyboardInterrupt:
                print("\nåœæ­¢è¿è¡Œã€‚")
                break
            except Exception as e:
                print(f"âŒ ä¸»å¾ªç¯é”™è¯¯: {e}")
                time.sleep(1)

    def _handle_command(self, text):
        self._llm_stop_event.clear()
        # è§†è§‰
        img_path = None
        if _need_vision_context(text):
            self.player.play("å¥½çš„ï¼Œæˆ‘çœ‹çœ‹ã€‚", blocking=False)
            img_path = _capture_image_file()
        
        # äººè„¸
        face_ctx = None
        if not img_path and _need_face_context(text):
            self.player.play("æ­£åœ¨è¯†åˆ«...", blocking=False)
            names = _get_face_names()
            if names:
                known = [n for n in names if n != "æœªçŸ¥"]
                if known: face_ctx = "ã€".join(known)
        
        # LLM æµå¼è°ƒç”¨
        # æ³¨æ„ï¼šå¦‚æœæœ‰ img_pathï¼Œç›®å‰ chat_with_llm ä¼šè‡ªåŠ¨é™çº§ä¸ºéæµå¼è¿”å› list
        stream_gen = chat_with_llm(text, face_context=face_ctx, image_path=img_path, stream=True)
        if img_path and os.path.exists(img_path): os.remove(img_path)
        
        print("AI: ", end="", flush=True)
        
        # åŒ…è£…ç”Ÿæˆå™¨ä»¥æ‰“å°è¾“å‡º
        def _printing_gen():
            for chunk in stream_gen:
                if self._llm_stop_event.is_set():
                    break
                print(chunk, end="", flush=True)
                yield chunk
            print("") # æ¢è¡Œ

        # æ’­æ”¾æµå¼éŸ³é¢‘
        # ä½¿ç”¨ zh-CN-YunyangNeural (æ–°é—»ç”·å£°) + rate="+10%" (è‡ªç„¶è¯­é€Ÿ) + pitch="-5Hz" æ¨¡æ‹Ÿæ²‰ç¨³è´¾ç»´æ–¯é£æ ¼
        self.player.play_stream(_printing_gen(), voice="zh-CN-YunyangNeural", rate="+10%", pitch="-5Hz", blocking=False)


def main():
    import argparse
    p = argparse.ArgumentParser()
    p.add_argument("--text", action="store_true")
    args = p.parse_args()
    
    if args.text:
        print("æ–‡å­—æ¨¡å¼...")
        while True:
            t = input("è¾“å…¥: ")
            reply, _ = chat_with_llm(t)
            print(f"AI: {reply}")
    else:
        app = SpeechAssistant()
        app.run()

if __name__ == "__main__":
    main()
